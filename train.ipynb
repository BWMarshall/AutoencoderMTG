{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c05a2943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f391144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cmc</th>\n",
       "      <th>colours</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>power</th>\n",
       "      <th>toughness</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+Two Mace</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 96, 0, 40, 186, 96, 114, 114, 10, 170, 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aarakocra Sneak</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 51, 31, 3, 0, 23, 4, 431, 9, 627, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aatchik, Emerald Radian</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>[0, 0, 1, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0,...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 31, 52, 23, 65, 5, 131, 162, 442, 0, 44, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abaddon the Despoiler</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>[0, 1, 1, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 126, 151, 11, 15, 30, 4, 29, 34, 11, 53, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abandoned Campground</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 3, 61, 23, 74, 159, 5, 37, 88, 688, 16, 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name     cmc          colours  \\\n",
       "0                +Two Mace  0.1250  [1, 0, 0, 0, 0]   \n",
       "1          Aarakocra Sneak  0.2500  [0, 1, 0, 0, 0]   \n",
       "2  Aatchik, Emerald Radian  0.3750  [0, 0, 1, 0, 1]   \n",
       "3    Abaddon the Despoiler  0.3125  [0, 1, 1, 1, 0]   \n",
       "4     Abandoned Campground  0.0000  [1, 1, 0, 0, 0]   \n",
       "\n",
       "                                      type  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                             subtype     power  toughness  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.000000       0.00   \n",
       "1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.055556       0.20   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0,...  0.166667       0.15   \n",
       "3  [0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.277778       0.25   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.000000       0.00   \n",
       "\n",
       "   loyalty                                             tokens  \n",
       "0      0.0  [1, 96, 0, 40, 186, 96, 114, 114, 10, 170, 12,...  \n",
       "1      0.0           [1, 51, 31, 3, 0, 23, 4, 431, 9, 627, 2]  \n",
       "2      0.0  [1, 31, 52, 23, 65, 5, 131, 162, 442, 0, 44, 4...  \n",
       "3      0.0  [1, 126, 151, 11, 15, 30, 4, 29, 34, 11, 53, 2...  \n",
       "4      0.0  [1, 3, 61, 23, 74, 159, 5, 37, 88, 688, 16, 12...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('./data/data.json', orient='records')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c9341d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class MTGCardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a single row from the dataframe\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Extract token indices (for the RNN)\n",
    "        original_tokens = row['tokens']\n",
    "        shifted_tokens = [t + 1 for t in original_tokens]  # Add 1 to all indices Allows for padding\n",
    "        tokens = torch.tensor(shifted_tokens, dtype=torch.long)\n",
    "        \n",
    "        # Extract numerical features (for the FC network)\n",
    "        numerical_features = []\n",
    "        \n",
    "        # Add CMC (converted mana cost)\n",
    "        numerical_features.append(row['cmc'])\n",
    "        \n",
    "        # Add other numerical features\n",
    "        numerical_features.extend(row['colours'])\n",
    "        numerical_features.extend(row['type'])\n",
    "        numerical_features.extend(row['subtype'])\n",
    "        numerical_features.append(row['power'])\n",
    "        numerical_features.append(row['toughness'])\n",
    "        numerical_features.append(row['loyalty'])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        numerical_features = torch.tensor(numerical_features, dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'numerical_features': numerical_features,\n",
    "            'name': row['name']\n",
    "        }\n",
    "\n",
    "dataset = MTGCardDataset(df)  # df is your pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e915e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([  2,  97,   1,  41, 187,  97, 115, 115,  11, 171,  13,  14,   1,   5,\n",
       "          15,  97, 100,  34,   6, 110,   3]),\n",
       " 'numerical_features': tensor([ 0.1250,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000]),\n",
       " 'name': '+Two Mace'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7ddb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_mtg_cards(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length token sequences.\n",
    "    \"\"\"\n",
    "    # Extract each element from the batch\n",
    "    names = [item['name'] for item in batch]\n",
    "    token_sequences = [item['tokens'] for item in batch]\n",
    "    numerical_features = [item['numerical_features'] for item in batch]\n",
    "    \n",
    "    # Get lengths of each sequence for packing\n",
    "    lengths = torch.tensor([len(seq) for seq in token_sequences], dtype=torch.long)\n",
    "    \n",
    "    # Sort sequences by length in descending order for efficient packing\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sorted_lengths = lengths[sorted_indices]\n",
    "    sorted_token_sequences = [token_sequences[i] for i in sorted_indices]\n",
    "    sorted_numerical_features = [numerical_features[i] for i in sorted_indices]\n",
    "    sorted_names = [names[i] for i in sorted_indices]\n",
    "    \n",
    "    # Pad token sequences\n",
    "    padded_tokens = pad_sequence(sorted_token_sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack numerical features\n",
    "    stacked_numerical_features = torch.stack(sorted_numerical_features)\n",
    "    \n",
    "    return {\n",
    "        'tokens': padded_tokens,\n",
    "        'token_lengths': sorted_lengths,\n",
    "        'numerical_features': stacked_numerical_features,\n",
    "        'names': sorted_names\n",
    "    }\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_mtg_cards\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e81b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_word2vec_embedding(model_path, padding_idx=0):\n",
    "    word2vec_model = Word2Vec.load(model_path)\n",
    "    \n",
    "    # Get embedding dimension from the model\n",
    "    embedding_dim = word2vec_model.vector_size\n",
    "    vocab_size = len(word2vec_model.wv)\n",
    "    \n",
    "    # Initialize embedding matrix with zeros\n",
    "    # Add 1 to vocab_size to account for padding token\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "    \n",
    "    # Fill the embedding matrix with word vectors\n",
    "    for i, word in enumerate(word2vec_model.wv.index_to_key):\n",
    "        # Add 1 to index to reserve index 0 for padding\n",
    "        embedding_matrix[i + 1] = word2vec_model.wv[word]\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    embedding_weights = torch.FloatTensor(embedding_matrix)\n",
    "    \n",
    "    # Create embedding layer initialized with pre-trained weights\n",
    "    embedding_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_weights,\n",
    "        padding_idx=padding_idx,\n",
    "        freeze=False  # Set to True if you don't want to fine-tune the embeddings\n",
    "    )\n",
    "    \n",
    "    return embedding_layer, vocab_size + 1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59ecf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MTGCardAutoencoderWithTextDecoding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_layer, hidden_dim, numerical_dim, latent_dim):\n",
    "        super(MTGCardAutoencoderWithTextDecoding, self).__init__()\n",
    "        \n",
    "        # Use pre-trained embedding layer\n",
    "        self.embedding = embedding_layer\n",
    "        embedding_dim = embedding_layer.embedding_dim\n",
    "        \n",
    "        # --- ENCODER COMPONENTS ---\n",
    "        # RNN for encoding token sequences\n",
    "        self.encoder_rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # FC network for processing numerical features\n",
    "        self.fc_numerical = nn.Sequential(\n",
    "            nn.Linear(numerical_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Combined encoder to latent space\n",
    "        self.fc_combined = nn.Linear(hidden_dim * 2, latent_dim)\n",
    "        \n",
    "        # --- DECODER COMPONENTS ---\n",
    "        # Decoder for numerical features\n",
    "        self.numerical_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, numerical_dim),\n",
    "            nn.Tanh()  # For normalized numerical features\n",
    "        )\n",
    "        \n",
    "        # Decoder for token sequences\n",
    "        self.decoder_initial = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_rnn = nn.GRU(embedding_dim + latent_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder_fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Store for later use\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def encode(self, tokens, token_lengths, numerical_features):\n",
    "        # Process token sequences with RNN\n",
    "        embedded = self.embedding(tokens)\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, token_lengths, batch_first=True\n",
    "        )\n",
    "        \n",
    "        _, rnn_hidden = self.encoder_rnn(packed)\n",
    "        rnn_hidden = rnn_hidden.squeeze(0)  # Shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Process numerical features\n",
    "        numerical_encoding = self.fc_numerical(numerical_features)\n",
    "        \n",
    "        # Combine both encodings\n",
    "        combined = torch.cat([rnn_hidden, numerical_encoding], dim=1)\n",
    "        latent = self.fc_combined(combined)\n",
    "        \n",
    "        return latent\n",
    "    \n",
    "    def decode_numerical(self, latent):\n",
    "        # Decode numerical features\n",
    "        return self.numerical_decoder(latent)\n",
    "    \n",
    "    def decode_tokens(self, latent, tokens, teacher_forcing_ratio=0.5):\n",
    "        batch_size = latent.size(0)\n",
    "        max_len = tokens.size(1)\n",
    "        \n",
    "        # Initialize tensor to store outputs\n",
    "        outputs = torch.zeros(batch_size, max_len, self.vocab_size).to(latent.device)\n",
    "        \n",
    "        # Initialize decoder hidden state from latent vector\n",
    "        hidden = self.decoder_initial(latent).unsqueeze(0)  # Add sequence dimension\n",
    "        \n",
    "        # First input is the special start token (we'll use zeros for simplicity)\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.embedding.embedding_dim).to(latent.device)\n",
    "        \n",
    "        # Expand latent vector to be concatenated with each input\n",
    "        expanded_latent = latent.unsqueeze(1).expand(-1, max_len, -1)\n",
    "        \n",
    "        for t in range(max_len):\n",
    "            # Concatenate decoder input with latent vector for each time step\n",
    "            decoder_input_with_latent = torch.cat([decoder_input, latent.unsqueeze(1)], dim=2)\n",
    "            \n",
    "            # Feed through RNN\n",
    "            output, hidden = self.decoder_rnn(decoder_input_with_latent, hidden)\n",
    "            \n",
    "            # Project to vocabulary space\n",
    "            output = self.decoder_fc(output.squeeze(1))\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Teacher forcing: use ground truth as next input with probability\n",
    "            use_teacher_forcing = (torch.rand(1).item() < teacher_forcing_ratio)\n",
    "            \n",
    "            if use_teacher_forcing and t < max_len - 1:\n",
    "                # Use actual next token from target\n",
    "                decoder_input = self.embedding(tokens[:, t+1].unsqueeze(1))\n",
    "            else:\n",
    "                # Use own prediction\n",
    "                top1 = output.max(1)[1]\n",
    "                decoder_input = self.embedding(top1.unsqueeze(1))\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, tokens, token_lengths, numerical_features, teacher_forcing_ratio=0.5):\n",
    "        # Encode input to latent space\n",
    "        latent = self.encode(tokens, token_lengths, numerical_features)\n",
    "        \n",
    "        # Decode numerical features\n",
    "        reconstructed_numerical = self.decode_numerical(latent)\n",
    "        \n",
    "        # Decode token sequence\n",
    "        reconstructed_tokens = self.decode_tokens(latent, tokens, teacher_forcing_ratio)\n",
    "        \n",
    "        return reconstructed_numerical, reconstructed_tokens, latent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a26d5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Total Loss: 1.859571, Numerical Loss: 0.033054, Token Loss: 3.686087\n",
      "Epoch [2/10], Total Loss: 1.142445, Numerical Loss: 0.015180, Token Loss: 2.269710\n",
      "Epoch [3/10], Total Loss: 0.931297, Numerical Loss: 0.011685, Token Loss: 1.850909\n",
      "Epoch [4/10], Total Loss: 0.827880, Numerical Loss: 0.009920, Token Loss: 1.645841\n",
      "Epoch [5/10], Total Loss: 0.766684, Numerical Loss: 0.008731, Token Loss: 1.524637\n",
      "Epoch [6/10], Total Loss: 0.710547, Numerical Loss: 0.007887, Token Loss: 1.413206\n",
      "Epoch [7/10], Total Loss: 0.682010, Numerical Loss: 0.007116, Token Loss: 1.356904\n",
      "Epoch [8/10], Total Loss: 0.648654, Numerical Loss: 0.006426, Token Loss: 1.290882\n",
      "Epoch [9/10], Total Loss: 0.625222, Numerical Loss: 0.005965, Token Loss: 1.244479\n",
      "Epoch [10/10], Total Loss: 0.599138, Numerical Loss: 0.005714, Token Loss: 1.192562\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 128\n",
    "numerical_dim = len(dataset[0]['numerical_features'])\n",
    "latent_dim = 64  # Desired embedding dimension\n",
    "\n",
    "embedding_layer, vocab_size = load_word2vec_embedding(\"./models/word2vec.model\")\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MTGCardAutoencoderWithTextDecoding(vocab_size, embedding_layer, hidden_dim, numerical_dim, latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss functions\n",
    "numerical_criterion = nn.MSELoss()\n",
    "token_criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens (index 0)\n",
    "alpha = 0.5  # Balance between numerical and token losses\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    numerical_losses = 0\n",
    "    token_losses = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        tokens = batch['tokens'].to(device)\n",
    "        token_lengths = batch['token_lengths']\n",
    "        numerical_features = batch['numerical_features'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed_numerical, reconstructed_tokens, latent = model(\n",
    "            tokens, token_lengths, numerical_features, teacher_forcing_ratio=0.5\n",
    "        )\n",
    "        \n",
    "        # Compute numerical reconstruction loss\n",
    "        numerical_loss = numerical_criterion(reconstructed_numerical, numerical_features)\n",
    "        \n",
    "        # Compute token reconstruction loss\n",
    "        # Reshape predictions to [batch_size*seq_len, vocab_size]\n",
    "        token_preds = reconstructed_tokens.view(-1, vocab_size)\n",
    "        # Reshape targets to [batch_size*seq_len]\n",
    "        token_targets = tokens.view(-1)\n",
    "        token_loss = token_criterion(token_preds, token_targets)\n",
    "        \n",
    "        # Combine losses        \n",
    "        loss = alpha * numerical_loss + (1 - alpha) * token_loss\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        numerical_losses += numerical_loss.item()\n",
    "        token_losses += token_loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Total Loss: {total_loss/len(dataloader):.6f}, \"\n",
    "          f\"Numerical Loss: {numerical_losses/len(dataloader):.6f}, \"\n",
    "          f\"Token Loss: {token_losses/len(dataloader):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all cards\n",
    "model.eval()\n",
    "all_embeddings = []\n",
    "all_names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        tokens = batch['tokens'].to(device)\n",
    "        token_lengths = batch['token_lengths']\n",
    "        numerical_features = batch['numerical_features'].to(device)\n",
    "        names = batch['names']\n",
    "        \n",
    "        _, _, latent = model(tokens, token_lengths, numerical_features)\n",
    "        \n",
    "        all_embeddings.append(latent.cpu().numpy())\n",
    "        all_names.extend(names)\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5cb38da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings and names to saved_embeddings/mtg_embeddings_20250724_125801.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_embeddings(embeddings, names, directory=\"saved_embeddings\", method=\"json\"):\n",
    "    \"\"\"\n",
    "    Save embeddings and corresponding card names using various methods\n",
    "    \n",
    "    Args:\n",
    "        embeddings: NumPy array of embeddings\n",
    "        names: List of card names\n",
    "        directory: Directory to save files in\n",
    "        method: 'pickle', 'numpy' or 'json'\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Create timestamp for the filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if method == \"pickle\":\n",
    "        # Save embeddings and names together in a dictionary\n",
    "        data = {\n",
    "            'embeddings': embeddings,\n",
    "            'names': names,\n",
    "            'metadata': {\n",
    "                'date': timestamp,\n",
    "                'embedding_dim': embeddings.shape[1],\n",
    "                'num_cards': len(names)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        filename = f\"{directory}/mtg_embeddings_{timestamp}.pkl\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Saved embeddings and names to {filename}\")\n",
    "        return filename\n",
    "        \n",
    "    elif method == \"numpy\":\n",
    "        # Save embeddings as .npy and names as pickle\n",
    "        emb_filename = f\"{directory}/mtg_embeddings_{timestamp}.npy\"\n",
    "        names_filename = f\"{directory}/mtg_names_{timestamp}.pkl\"\n",
    "        \n",
    "        np.save(emb_filename, embeddings)\n",
    "        with open(names_filename, 'wb') as f:\n",
    "            pickle.dump(names, f)\n",
    "            \n",
    "        print(f\"Saved embeddings to {emb_filename} and names to {names_filename}\")\n",
    "        return emb_filename, names_filename\n",
    "        \n",
    "\n",
    "    elif method == \"json\":\n",
    "        # Save as JSON (less efficient but human-readable)\n",
    "        filename = f\"{directory}/mtg_embeddings_{timestamp}.json\"\n",
    "        \n",
    "        data = {\n",
    "            'embeddings': embeddings.tolist(),\n",
    "            'names': names,\n",
    "            'metadata': {\n",
    "                'date': timestamp,\n",
    "                'embedding_dim': embeddings.shape[1],\n",
    "                'num_cards': len(names)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "            \n",
    "        print(f\"Saved embeddings and names to {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pickle', 'numpy', or 'json'\")\n",
    "\n",
    "# Example usage:\n",
    "save_path = save_embeddings(all_embeddings, all_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
