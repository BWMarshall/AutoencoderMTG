{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05a2943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f391144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cmc</th>\n",
       "      <th>colours</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>power</th>\n",
       "      <th>toughness</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+Two Mace</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 96, 0, 40, 186, 96, 114, 114, 10, 170, 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aarakocra Sneak</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 51, 31, 3, 0, 23, 4, 431, 9, 627, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aatchik, Emerald Radian</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>[0, 0, 1, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0,...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 31, 52, 23, 65, 5, 131, 162, 442, 0, 44, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abaddon the Despoiler</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>[0, 1, 1, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>[0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 126, 151, 11, 15, 30, 4, 29, 34, 11, 53, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abandoned Campground</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 3, 61, 23, 74, 159, 5, 37, 88, 688, 16, 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name     cmc          colours  \\\n",
       "0                +Two Mace  0.1250  [1, 0, 0, 0, 0]   \n",
       "1          Aarakocra Sneak  0.2500  [0, 1, 0, 0, 0]   \n",
       "2  Aatchik, Emerald Radian  0.3750  [0, 0, 1, 0, 1]   \n",
       "3    Abaddon the Despoiler  0.3125  [0, 1, 1, 1, 0]   \n",
       "4     Abandoned Campground  0.0000  [1, 1, 0, 0, 0]   \n",
       "\n",
       "                                      type  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "3  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                             subtype     power  toughness  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.000000       0.00   \n",
       "1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.055556       0.20   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0,...  0.166667       0.15   \n",
       "3  [0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.277778       0.25   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.000000       0.00   \n",
       "\n",
       "   loyalty                                             tokens  \n",
       "0      0.0  [1, 96, 0, 40, 186, 96, 114, 114, 10, 170, 12,...  \n",
       "1      0.0           [1, 51, 31, 3, 0, 23, 4, 431, 9, 627, 2]  \n",
       "2      0.0  [1, 31, 52, 23, 65, 5, 131, 162, 442, 0, 44, 4...  \n",
       "3      0.0  [1, 126, 151, 11, 15, 30, 4, 29, 34, 11, 53, 2...  \n",
       "4      0.0  [1, 3, 61, 23, 74, 159, 5, 37, 88, 688, 16, 12...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('./data/data.json', orient='records')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9341d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class MTGCardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a single row from the dataframe\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Extract token indices (for the RNN)\n",
    "        original_tokens = row['tokens']\n",
    "        shifted_tokens = [t + 1 for t in original_tokens]  # Add 1 to all indices Allows for padding\n",
    "        tokens = torch.tensor(shifted_tokens, dtype=torch.long)\n",
    "        \n",
    "        # Extract numerical features (for the FC network)\n",
    "        numerical_features = []\n",
    "        \n",
    "        # Add CMC (converted mana cost)\n",
    "        numerical_features.append(row['cmc'])\n",
    "        \n",
    "        # Add other numerical features\n",
    "        numerical_features.extend(row['colours'])\n",
    "        numerical_features.extend(row['type'])\n",
    "        numerical_features.extend(row['subtype'])\n",
    "        numerical_features.append(row['power'])\n",
    "        numerical_features.append(row['toughness'])\n",
    "        numerical_features.append(row['loyalty'])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        numerical_features = torch.tensor(numerical_features, dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'numerical_features': numerical_features,\n",
    "            'name': row['name']\n",
    "        }\n",
    "\n",
    "dataset = MTGCardDataset(df)  # df is your pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e915e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([  2,  97,   1,  41, 187,  97, 115, 115,  11, 171,  13,  14,   1,   5,\n",
       "          15,  97, 100,  34,   6, 110,   3]),\n",
       " 'numerical_features': tensor([ 0.1250,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, -1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000]),\n",
       " 'name': '+Two Mace'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ddb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_mtg_cards(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length token sequences.\n",
    "    \"\"\"\n",
    "    # Extract each element from the batch\n",
    "    names = [item['name'] for item in batch]\n",
    "    token_sequences = [item['tokens'] for item in batch]\n",
    "    numerical_features = [item['numerical_features'] for item in batch]\n",
    "    \n",
    "    # Get lengths of each sequence for packing\n",
    "    lengths = torch.tensor([len(seq) for seq in token_sequences], dtype=torch.long)\n",
    "    \n",
    "    # Sort sequences by length in descending order for efficient packing\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sorted_lengths = lengths[sorted_indices]\n",
    "    sorted_token_sequences = [token_sequences[i] for i in sorted_indices]\n",
    "    sorted_numerical_features = [numerical_features[i] for i in sorted_indices]\n",
    "    sorted_names = [names[i] for i in sorted_indices]\n",
    "    \n",
    "    # Pad token sequences\n",
    "    padded_tokens = pad_sequence(sorted_token_sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack numerical features\n",
    "    stacked_numerical_features = torch.stack(sorted_numerical_features)\n",
    "    \n",
    "    return {\n",
    "        'tokens': padded_tokens,\n",
    "        'token_lengths': sorted_lengths,\n",
    "        'numerical_features': stacked_numerical_features,\n",
    "        'names': sorted_names\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0298bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Now create your datasets\n",
    "train_dataset = MTGCardDataset(train_df)\n",
    "val_dataset = MTGCardDataset(val_df)\n",
    "\n",
    "# Create your dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_mtg_cards\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_mtg_cards\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e81b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_word2vec_embedding(model_path, padding_idx=0):\n",
    "    word2vec_model = Word2Vec.load(model_path)\n",
    "    \n",
    "    # Get embedding dimension from the model\n",
    "    embedding_dim = word2vec_model.vector_size\n",
    "    vocab_size = len(word2vec_model.wv)\n",
    "    \n",
    "    # Initialize embedding matrix with zeros\n",
    "    # Add 1 to vocab_size to account for padding token\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "    \n",
    "    # Fill the embedding matrix with word vectors\n",
    "    for i, word in enumerate(word2vec_model.wv.index_to_key):\n",
    "        # Add 1 to index to reserve index 0 for padding\n",
    "        embedding_matrix[i + 1] = word2vec_model.wv[word]\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    embedding_weights = torch.FloatTensor(embedding_matrix)\n",
    "    \n",
    "    # Create embedding layer initialized with pre-trained weights\n",
    "    embedding_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_weights,\n",
    "        padding_idx=padding_idx,\n",
    "        freeze=False  # Set to True if you don't want to fine-tune the embeddings\n",
    "    )\n",
    "    \n",
    "    return embedding_layer, vocab_size + 1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ecf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MTGCardAutoencoderWithTextDecoding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_layer, hidden_dim, numerical_dim, latent_dim):\n",
    "        super(MTGCardAutoencoderWithTextDecoding, self).__init__()\n",
    "        \n",
    "        # Use pre-trained embedding layer\n",
    "        self.embedding = embedding_layer\n",
    "        embedding_dim = embedding_layer.embedding_dim\n",
    "        \n",
    "        # --- ENCODER COMPONENTS ---\n",
    "        # RNN for encoding token sequences\n",
    "        self.encoder_rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # FC network for processing numerical features\n",
    "        self.fc_numerical = nn.Sequential(\n",
    "            nn.Linear(numerical_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Combined encoder to latent space\n",
    "        self.fc_combined = nn.Linear(hidden_dim * 2, latent_dim)\n",
    "        \n",
    "        # --- DECODER COMPONENTS ---\n",
    "        # Decoder for numerical features\n",
    "        self.numerical_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, numerical_dim),\n",
    "            nn.Tanh()  # For normalized numerical features\n",
    "        )\n",
    "        \n",
    "        # Decoder for token sequences\n",
    "        self.decoder_initial = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_rnn = nn.GRU(embedding_dim + latent_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder_fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Store for later use\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def encode(self, tokens, token_lengths, numerical_features):\n",
    "        # Process token sequences with RNN\n",
    "        embedded = self.embedding(tokens)\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, token_lengths, batch_first=True\n",
    "        )\n",
    "        \n",
    "        _, rnn_hidden = self.encoder_rnn(packed)\n",
    "        rnn_hidden = rnn_hidden.squeeze(0)  # Shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Process numerical features\n",
    "        numerical_encoding = self.fc_numerical(numerical_features)\n",
    "        \n",
    "        # Combine both encodings\n",
    "        combined = torch.cat([rnn_hidden, numerical_encoding], dim=1)\n",
    "        latent = self.fc_combined(combined)\n",
    "        \n",
    "        return latent\n",
    "    \n",
    "    def decode_numerical(self, latent):\n",
    "        # Decode numerical features\n",
    "        return self.numerical_decoder(latent)\n",
    "    \n",
    "    def decode_tokens(self, latent, tokens, teacher_forcing_ratio=0.5):\n",
    "        batch_size = latent.size(0)\n",
    "        max_len = tokens.size(1)\n",
    "        \n",
    "        # Initialize tensor to store outputs\n",
    "        outputs = torch.zeros(batch_size, max_len, self.vocab_size).to(latent.device)\n",
    "        \n",
    "        # Initialize decoder hidden state from latent vector\n",
    "        hidden = self.decoder_initial(latent).unsqueeze(0)  # Add sequence dimension\n",
    "        \n",
    "        # First input is the special start token (we'll use zeros for simplicity)\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.embedding.embedding_dim).to(latent.device)\n",
    "        \n",
    "        # Expand latent vector to be concatenated with each input\n",
    "        expanded_latent = latent.unsqueeze(1).expand(-1, max_len, -1)\n",
    "        \n",
    "        for t in range(max_len):\n",
    "            # Concatenate decoder input with latent vector for each time step\n",
    "            decoder_input_with_latent = torch.cat([decoder_input, latent.unsqueeze(1)], dim=2)\n",
    "            \n",
    "            # Feed through RNN\n",
    "            output, hidden = self.decoder_rnn(decoder_input_with_latent, hidden)\n",
    "            \n",
    "            # Project to vocabulary space\n",
    "            output = self.decoder_fc(output.squeeze(1))\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Teacher forcing: use ground truth as next input with probability\n",
    "            use_teacher_forcing = (torch.rand(1).item() < teacher_forcing_ratio)\n",
    "            \n",
    "            if use_teacher_forcing and t < max_len - 1:\n",
    "                # Use actual next token from target\n",
    "                decoder_input = self.embedding(tokens[:, t+1].unsqueeze(1))\n",
    "            else:\n",
    "                # Use own prediction\n",
    "                top1 = output.max(1)[1]\n",
    "                decoder_input = self.embedding(top1.unsqueeze(1))\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, tokens, token_lengths, numerical_features, teacher_forcing_ratio=0.5):\n",
    "        # Encode input to latent space\n",
    "        latent = self.encode(tokens, token_lengths, numerical_features)\n",
    "        \n",
    "        # Decode numerical features\n",
    "        reconstructed_numerical = self.decode_numerical(latent)\n",
    "        \n",
    "        # Decode token sequence\n",
    "        reconstructed_tokens = self.decode_tokens(latent, tokens, teacher_forcing_ratio)\n",
    "        \n",
    "        return reconstructed_numerical, reconstructed_tokens, latent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb988f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 128\n",
    "numerical_dim = len(dataset[0]['numerical_features'])\n",
    "latent_dim = 64  # Desired embedding dimension\n",
    "\n",
    "embedding_layer, vocab_size = load_word2vec_embedding(\"./models/word2vec.model\")\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MTGCardAutoencoderWithTextDecoding(vocab_size, embedding_layer, hidden_dim, numerical_dim, latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss functions\n",
    "numerical_criterion = nn.MSELoss()\n",
    "token_criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens (index 0)\n",
    "alpha = 0.5  # Balance between numerical and token losses\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "def evaluate(model, val_dataloader, numerical_criterion, token_criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # Extract inputs from batch\n",
    "            tokens = batch['tokens'].to(device)\n",
    "            token_lengths = batch['token_lengths']\n",
    "            numerical_features = batch['numerical_features'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed_numerical, reconstructed_tokens, latent = model(\n",
    "                tokens, token_lengths, numerical_features, teacher_forcing_ratio=0.5\n",
    "            )\n",
    "            \n",
    "            # Compute numerical reconstruction loss\n",
    "            numerical_loss = numerical_criterion(reconstructed_numerical, numerical_features)\n",
    "            \n",
    "            # Compute token reconstruction loss\n",
    "            token_preds = reconstructed_tokens.view(-1, vocab_size)\n",
    "            token_targets = tokens.view(-1)\n",
    "            token_loss = token_criterion(token_preds, token_targets)\n",
    "            \n",
    "            # Combine losses\n",
    "            loss = alpha * numerical_loss + (1 - alpha) * token_loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a26d5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/50], Train Loss: 1.884842 - (Numerical: 0.034641, Token: 3.735042),  Val Loss:1.373214\n",
      "\n",
      "Epoch [2/50], Train Loss: 1.168256 - (Numerical: 0.016346, Token: 2.320165),  Val Loss:1.052724\n",
      "\n",
      "Epoch [3/50], Train Loss: 0.944260 - (Numerical: 0.011963, Token: 1.876557),  Val Loss:0.911036\n",
      "\n",
      "Epoch [4/50], Train Loss: 0.843249 - (Numerical: 0.009874, Token: 1.676625),  Val Loss:0.783415\n",
      "\n",
      "Epoch [5/50], Train Loss: 0.778206 - (Numerical: 0.008637, Token: 1.547774),  Val Loss:0.777831\n",
      "\n",
      "Epoch [6/50], Train Loss: 0.721446 - (Numerical: 0.007860, Token: 1.435031),  Val Loss:0.750574\n",
      "\n",
      "Epoch [7/50], Train Loss: 0.694269 - (Numerical: 0.007215, Token: 1.381322),  Val Loss:0.703223\n",
      "\n",
      "Epoch [8/50], Train Loss: 0.660767 - (Numerical: 0.006624, Token: 1.314911),  Val Loss:0.676384\n",
      "\n",
      "Epoch [9/50], Train Loss: 0.631860 - (Numerical: 0.006307, Token: 1.257413),  Val Loss:0.657009\n",
      "\n",
      "Epoch [10/50], Train Loss: 0.602680 - (Numerical: 0.005794, Token: 1.199566),  Val Loss:0.617204\n",
      "\n",
      "Epoch [11/50], Train Loss: 0.594713 - (Numerical: 0.005565, Token: 1.183861),  Val Loss:0.616387\n",
      "\n",
      "Epoch [12/50], Train Loss: 0.582726 - (Numerical: 0.005409, Token: 1.160044),  Val Loss:0.612834\n",
      "\n",
      "Epoch [13/50], Train Loss: 0.569436 - (Numerical: 0.005192, Token: 1.133680),  Val Loss:0.567427\n",
      "\n",
      "Epoch [14/50], Train Loss: 0.557107 - (Numerical: 0.005090, Token: 1.109124),  Val Loss:0.592622\n",
      "\n",
      "Epoch [15/50], Train Loss: 0.551268 - (Numerical: 0.005021, Token: 1.097516),  Val Loss:0.575055\n",
      "\n",
      "Epoch [16/50], Train Loss: 0.535169 - (Numerical: 0.004918, Token: 1.065419),  Val Loss:0.567488\n",
      "\n",
      "Epoch [17/50], Train Loss: 0.525218 - (Numerical: 0.004813, Token: 1.045624),  Val Loss:0.573872\n",
      "\n",
      "Epoch [18/50], Train Loss: 0.520974 - (Numerical: 0.004732, Token: 1.037215),  Val Loss:0.560619\n",
      "\n",
      "Epoch [19/50], Train Loss: 0.500349 - (Numerical: 0.004627, Token: 0.996071),  Val Loss:0.536223\n",
      "\n",
      "Epoch [20/50], Train Loss: 0.509320 - (Numerical: 0.004606, Token: 1.014034),  Val Loss:0.564591\n",
      "\n",
      "Epoch [21/50], Train Loss: 0.496811 - (Numerical: 0.004589, Token: 0.989032),  Val Loss:0.548260\n",
      "\n",
      "Epoch [22/50], Train Loss: 0.498583 - (Numerical: 0.004553, Token: 0.992612),  Val Loss:0.541639\n",
      "\n",
      "Epoch [23/50], Train Loss: 0.487231 - (Numerical: 0.004503, Token: 0.969960),  Val Loss:0.528789\n",
      "\n",
      "Epoch [24/50], Train Loss: 0.485875 - (Numerical: 0.004564, Token: 0.967185),  Val Loss:0.532155\n",
      "\n",
      "Epoch [25/50], Train Loss: 0.481042 - (Numerical: 0.004612, Token: 0.957471),  Val Loss:0.525535\n",
      "\n",
      "Epoch [26/50], Train Loss: 0.478979 - (Numerical: 0.004443, Token: 0.953515),  Val Loss:0.545280\n",
      "\n",
      "Epoch [27/50], Train Loss: 0.467050 - (Numerical: 0.004518, Token: 0.929582),  Val Loss:0.539259\n",
      "\n",
      "Epoch [28/50], Train Loss: 0.467963 - (Numerical: 0.004488, Token: 0.931437),  Val Loss:0.535691\n",
      "\n",
      "Epoch [29/50], Train Loss: 0.457945 - (Numerical: 0.004426, Token: 0.911465),  Val Loss:0.524812\n",
      "\n",
      "Epoch [30/50], Train Loss: 0.456455 - (Numerical: 0.004537, Token: 0.908373),  Val Loss:0.504477\n",
      "\n",
      "Epoch [31/50], Train Loss: 0.448165 - (Numerical: 0.004499, Token: 0.891832),  Val Loss:0.497282\n",
      "\n",
      "Epoch [32/50], Train Loss: 0.460128 - (Numerical: 0.004582, Token: 0.915675),  Val Loss:0.469505\n",
      "\n",
      "Epoch [33/50], Train Loss: 0.451323 - (Numerical: 0.004618, Token: 0.898028),  Val Loss:0.494485\n",
      "\n",
      "Epoch [34/50], Train Loss: 0.448454 - (Numerical: 0.004495, Token: 0.892412),  Val Loss:0.485224\n",
      "\n",
      "Epoch [35/50], Train Loss: 0.437474 - (Numerical: 0.004401, Token: 0.870547),  Val Loss:0.484843\n",
      "\n",
      "Epoch [36/50], Train Loss: 0.447199 - (Numerical: 0.004514, Token: 0.889885),  Val Loss:0.507614\n",
      "\n",
      "Epoch [37/50], Train Loss: 0.435120 - (Numerical: 0.004263, Token: 0.865977),  Val Loss:0.477063\n",
      "\n",
      "Epoch [38/50], Train Loss: 0.438457 - (Numerical: 0.004318, Token: 0.872596),  Val Loss:0.491196\n",
      "\n",
      "Epoch [39/50], Train Loss: 0.434444 - (Numerical: 0.004332, Token: 0.864555),  Val Loss:0.496697\n",
      "\n",
      "Epoch [40/50], Train Loss: 0.430893 - (Numerical: 0.004226, Token: 0.857561),  Val Loss:0.493896\n",
      "\n",
      "Epoch [41/50], Train Loss: 0.422518 - (Numerical: 0.004253, Token: 0.840783),  Val Loss:0.492965\n",
      "\n",
      "Epoch [42/50], Train Loss: 0.424135 - (Numerical: 0.004365, Token: 0.843905),  Val Loss:0.475628\n",
      "\n",
      "Epoch [43/50], Train Loss: 0.425896 - (Numerical: 0.004212, Token: 0.847581),  Val Loss:0.482420\n",
      "\n",
      "Epoch [44/50], Train Loss: 0.426599 - (Numerical: 0.004385, Token: 0.848814),  Val Loss:0.496333\n",
      "\n",
      "Epoch [45/50], Train Loss: 0.424725 - (Numerical: 0.004376, Token: 0.845075),  Val Loss:0.478188\n",
      "\n",
      "Epoch [46/50], Train Loss: 0.421810 - (Numerical: 0.004210, Token: 0.839409),  Val Loss:0.473520\n",
      "\n",
      "Epoch [47/50], Train Loss: 0.413434 - (Numerical: 0.004251, Token: 0.822616),  Val Loss:0.469152\n",
      "\n",
      "Epoch [48/50], Train Loss: 0.411560 - (Numerical: 0.004266, Token: 0.818853),  Val Loss:0.470490\n",
      "\n",
      "Epoch [49/50], Train Loss: 0.422712 - (Numerical: 0.004615, Token: 0.840808),  Val Loss:0.478989\n",
      "\n",
      "Epoch [50/50], Train Loss: 0.417026 - (Numerical: 0.004299, Token: 0.829752),  Val Loss:0.479426\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    numerical_losses = 0\n",
    "    token_losses = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        tokens = batch['tokens'].to(device)\n",
    "        token_lengths = batch['token_lengths']\n",
    "        numerical_features = batch['numerical_features'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed_numerical, reconstructed_tokens, latent = model(\n",
    "            tokens, token_lengths, numerical_features, teacher_forcing_ratio=0.5\n",
    "        )\n",
    "        \n",
    "        # Compute numerical reconstruction loss\n",
    "        numerical_loss = numerical_criterion(reconstructed_numerical, numerical_features)\n",
    "        \n",
    "        # Compute token reconstruction loss\n",
    "        # Reshape predictions to [batch_size*seq_len, vocab_size]\n",
    "        token_preds = reconstructed_tokens.view(-1, vocab_size)\n",
    "        # Reshape targets to [batch_size*seq_len]\n",
    "        token_targets = tokens.view(-1)\n",
    "        token_loss = token_criterion(token_preds, token_targets)\n",
    "        \n",
    "        # Combine losses        \n",
    "        loss = alpha * numerical_loss + (1 - alpha) * token_loss\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        numerical_losses += numerical_loss.item()\n",
    "        token_losses += token_loss.item()\n",
    "    \n",
    "    val_loss = evaluate(model, val_dataloader, numerical_criterion, token_criterion, device)\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {total_loss/len(train_dataloader):.6f} - (\"\n",
    "          f\"Numerical: {numerical_losses/len(train_dataloader):.6f}, \"\n",
    "          f\"Token: {token_losses/len(train_dataloader):.6f}), \"\n",
    "          f\" Val Loss:{val_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0af10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/AE_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c1f4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all cards\n",
    "model.eval()\n",
    "all_embeddings = []\n",
    "all_names = []\n",
    "\n",
    "dataset = MTGCardDataset(df)  # df is your pandas dataframe\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_mtg_cards\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        tokens = batch['tokens'].to(device)\n",
    "        token_lengths = batch['token_lengths']\n",
    "        numerical_features = batch['numerical_features'].to(device)\n",
    "        names = batch['names']\n",
    "        \n",
    "        _, _, latent = model(tokens, token_lengths, numerical_features)\n",
    "        \n",
    "        all_embeddings.append(latent.cpu().numpy())\n",
    "        all_names.extend(names)\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb38da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings and names to saved_embeddings/mtg_embeddings_20250728_153743.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_embeddings(embeddings, names, directory=\"saved_embeddings\"):\n",
    "    \"\"\"\n",
    "    Save embeddings and corresponding card names using various methods\n",
    "    \n",
    "    Args:\n",
    "        embeddings: NumPy array of embeddings\n",
    "        names: List of card names\n",
    "        directory: Directory to save files in\n",
    "    \"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{directory}/mtg_embeddings_{timestamp}.json\"\n",
    "    \n",
    "    data = {\n",
    "        'embeddings': embeddings.tolist(),\n",
    "        'names': names,\n",
    "        'metadata': {\n",
    "            'date': timestamp,\n",
    "            'embedding_dim': embeddings.shape[1],\n",
    "            'num_cards': len(names)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    print(f\"Saved embeddings and names to {filename}\")\n",
    "    return filename\n",
    "\n",
    "save_path = save_embeddings(all_embeddings, all_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
