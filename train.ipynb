{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05a2943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f391144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>img</th>\n",
       "      <th>cmc</th>\n",
       "      <th>colours</th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>power</th>\n",
       "      <th>toughness</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+Two Mace</td>\n",
       "      <td>https://cards.scryfall.io/normal/front/e/8/e88...</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 96, 0, 40, 186, 96, 116, 116, 10, 169, 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aarakocra Sneak</td>\n",
       "      <td>https://cards.scryfall.io/normal/front/2/a/2a8...</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 50, 30, 3, 0, 23, 4, 428, 9, 629, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aatchik, Emerald Radian</td>\n",
       "      <td>https://cards.scryfall.io/normal/front/f/b/fbd...</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>[0, 0, 1, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0,...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 30, 52, 23, 65, 5, 131, 162, 430, 0, 44, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abaddon the Despoiler</td>\n",
       "      <td>https://cards.scryfall.io/normal/front/c/9/c9f...</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>[0, 1, 1, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 128, 152, 11, 15, 31, 4, 29, 34, 11, 53, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abandoned Campground</td>\n",
       "      <td>https://cards.scryfall.io/normal/front/e/e/ee0...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[1, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 3, 60, 23, 74, 159, 5, 37, 88, 677, 16, 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                                                img  \\\n",
       "0                +Two Mace  https://cards.scryfall.io/normal/front/e/8/e88...   \n",
       "1          Aarakocra Sneak  https://cards.scryfall.io/normal/front/2/a/2a8...   \n",
       "2  Aatchik, Emerald Radian  https://cards.scryfall.io/normal/front/f/b/fbd...   \n",
       "3    Abaddon the Despoiler  https://cards.scryfall.io/normal/front/c/9/c9f...   \n",
       "4     Abandoned Campground  https://cards.scryfall.io/normal/front/e/e/ee0...   \n",
       "\n",
       "      cmc          colours                                               type  \\\n",
       "0  0.1250  [1, 0, 0, 0, 0]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  0.2500  [0, 1, 0, 0, 0]  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  0.3750  [0, 0, 1, 0, 1]  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  0.3125  [0, 1, 1, 1, 0]  [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  0.0000  [1, 1, 0, 0, 0]  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             subtype  power  toughness  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   0.00       0.00   \n",
       "1  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   0.05       0.20   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 1.0,...   0.15       0.15   \n",
       "3  [0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   0.25       0.25   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   0.00       0.00   \n",
       "\n",
       "   loyalty                                             tokens  \n",
       "0      0.0  [1, 96, 0, 40, 186, 96, 116, 116, 10, 169, 12,...  \n",
       "1      0.0           [1, 50, 30, 3, 0, 23, 4, 428, 9, 629, 2]  \n",
       "2      0.0  [1, 30, 52, 23, 65, 5, 131, 162, 430, 0, 44, 4...  \n",
       "3      0.0  [1, 128, 152, 11, 15, 31, 4, 29, 34, 11, 53, 2...  \n",
       "4      0.0  [1, 3, 60, 23, 74, 159, 5, 37, 88, 677, 16, 12...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('./data/data.json', orient='records')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9341d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGCardDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a single row from the dataframe\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Extract token indices (for the RNN)\n",
    "        original_tokens = row['tokens']\n",
    "        shifted_tokens = [t + 1 for t in original_tokens]  # Add 1 to all indices Allows for padding\n",
    "        tokens = torch.tensor(shifted_tokens, dtype=torch.long)\n",
    "        \n",
    "        # Extract numerical features (for the FC network)\n",
    "        numerical_features = []\n",
    "        \n",
    "        # Add CMC (converted mana cost)\n",
    "        numerical_features.append(row['cmc'])\n",
    "        \n",
    "        # Add other numerical features\n",
    "        numerical_features.extend(row['colours'])\n",
    "        numerical_features.extend(row['type'])\n",
    "        numerical_features.extend(row['subtype'])\n",
    "        numerical_features.append(row['power'])\n",
    "        numerical_features.append(row['toughness'])\n",
    "        numerical_features.append(row['loyalty'])\n",
    "        \n",
    "        # Convert to tensor\n",
    "        numerical_features = torch.tensor(numerical_features, dtype=torch.float)\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'numerical_features': numerical_features,\n",
    "            'name': row['name'],\n",
    "            'img': row['img']\n",
    "        }\n",
    "\n",
    "dataset = MTGCardDataset(df)  # df is your pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e915e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([  2,  97,   1,  41, 187,  97, 117, 117,  11, 170,  13,  14,   1,   5,\n",
       "          15,  97, 100,  34,   6, 109,   3]),\n",
       " 'numerical_features': tensor([ 0.1250,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000]),\n",
       " 'name': '+Two Mace',\n",
       " 'img': 'https://cards.scryfall.io/normal/front/e/8/e882c9f9-bf30-46b6-bedc-379d2c80e5cb.jpg?1627701221'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ddb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_mtg_cards(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length token sequences.\n",
    "    \"\"\"\n",
    "    # Extract each element from the batch\n",
    "    names = [item['name'] for item in batch]\n",
    "    imgs = [item['img'] for item in batch]\n",
    "    token_sequences = [item['tokens'] for item in batch]\n",
    "    numerical_features = [item['numerical_features'] for item in batch]\n",
    "    \n",
    "    # Get lengths of each sequence for packing\n",
    "    lengths = torch.tensor([len(seq) for seq in token_sequences], dtype=torch.long)\n",
    "    \n",
    "    # Sort sequences by length in descending order for efficient packing\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sorted_lengths = lengths[sorted_indices]\n",
    "    sorted_token_sequences = [token_sequences[i] for i in sorted_indices]\n",
    "    sorted_numerical_features = [numerical_features[i] for i in sorted_indices]\n",
    "    sorted_names = [names[i] for i in sorted_indices]\n",
    "    sorted_imgs = [imgs[i] for i in sorted_indices]\n",
    "    \n",
    "    # Pad token sequences\n",
    "    padded_tokens = pad_sequence(sorted_token_sequences, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack numerical features\n",
    "    stacked_numerical_features = torch.stack(sorted_numerical_features)\n",
    "    \n",
    "    return {\n",
    "        'tokens': padded_tokens,\n",
    "        'token_lengths': sorted_lengths,\n",
    "        'numerical_features': stacked_numerical_features,\n",
    "        'names': sorted_names,\n",
    "        'imgs': sorted_imgs\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0298bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Now create your datasets\n",
    "train_dataset = MTGCardDataset(train_df)\n",
    "val_dataset = MTGCardDataset(val_df)\n",
    "\n",
    "# Create your dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_mtg_cards\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_mtg_cards\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e81b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec_embedding(model_path, padding_idx=0):\n",
    "    word2vec_model = Word2Vec.load(model_path)\n",
    "    \n",
    "    # Get embedding dimension from the model\n",
    "    embedding_dim = word2vec_model.vector_size\n",
    "    vocab_size = len(word2vec_model.wv)\n",
    "    \n",
    "    # Initialize embedding matrix with zeros\n",
    "    # Add 1 to vocab_size to account for padding token\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "    \n",
    "    # Fill the embedding matrix with word vectors\n",
    "    for i, word in enumerate(word2vec_model.wv.index_to_key):\n",
    "        # Add 1 to index to reserve index 0 for padding\n",
    "        embedding_matrix[i + 1] = word2vec_model.wv[word]\n",
    "    \n",
    "    # Convert to torch tensor\n",
    "    embedding_weights = torch.FloatTensor(embedding_matrix)\n",
    "    \n",
    "    # Create embedding layer initialized with pre-trained weights\n",
    "    embedding_layer = nn.Embedding.from_pretrained(\n",
    "        embedding_weights,\n",
    "        padding_idx=padding_idx,\n",
    "        freeze=False  # Set to True if you don't want to fine-tune the embeddings\n",
    "    )\n",
    "    \n",
    "    return embedding_layer, vocab_size + 1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ecf914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGCardAutoencoderWithTextDecoding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_layer, hidden_dim, numerical_dim, latent_dim):\n",
    "        super(MTGCardAutoencoderWithTextDecoding, self).__init__()\n",
    "        \n",
    "        # Use pre-trained embedding layer\n",
    "        self.embedding = embedding_layer\n",
    "        embedding_dim = embedding_layer.embedding_dim\n",
    "        \n",
    "        # --- ENCODER COMPONENTS ---\n",
    "        # RNN for encoding token sequences\n",
    "        self.encoder_rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # FC network for processing numerical features\n",
    "        self.fc_numerical = nn.Sequential(\n",
    "            nn.Linear(numerical_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Combined encoder to latent space\n",
    "        self.fc_combined = nn.Linear(hidden_dim * 2, latent_dim)\n",
    "        \n",
    "        # --- DECODER COMPONENTS ---\n",
    "        # Decoder for numerical features\n",
    "        self.numerical_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, numerical_dim),\n",
    "            nn.Tanh()  # For normalized numerical features\n",
    "        )\n",
    "        \n",
    "        # Decoder for token sequences\n",
    "        self.decoder_initial = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_rnn = nn.GRU(embedding_dim + latent_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder_fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Store for later use\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def encode(self, tokens, token_lengths, numerical_features):\n",
    "        # Process token sequences with RNN\n",
    "        embedded = self.embedding(tokens)\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, token_lengths, batch_first=True\n",
    "        )\n",
    "        \n",
    "        _, rnn_hidden = self.encoder_rnn(packed)\n",
    "        rnn_hidden = rnn_hidden.squeeze(0)  # Shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Process numerical features\n",
    "        numerical_encoding = self.fc_numerical(numerical_features)\n",
    "        \n",
    "        # Combine both encodings\n",
    "        combined = torch.cat([rnn_hidden, numerical_encoding], dim=1)\n",
    "        latent = self.fc_combined(combined)\n",
    "        \n",
    "        return latent\n",
    "    \n",
    "    def decode_numerical(self, latent):\n",
    "        # Decode numerical features\n",
    "        return self.numerical_decoder(latent)\n",
    "    \n",
    "    def decode_tokens(self, latent, tokens, teacher_forcing_ratio=0.5):\n",
    "        batch_size = latent.size(0)\n",
    "        max_len = tokens.size(1)\n",
    "        \n",
    "        # Initialize tensor to store outputs\n",
    "        outputs = torch.zeros(batch_size, max_len, self.vocab_size).to(latent.device)\n",
    "        \n",
    "        # Initialize decoder hidden state from latent vector\n",
    "        hidden = self.decoder_initial(latent).unsqueeze(0)  # Add sequence dimension\n",
    "        \n",
    "        # First input is the special start token (we'll use zeros for simplicity)\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.embedding.embedding_dim).to(latent.device)\n",
    "        \n",
    "        # Expand latent vector to be concatenated with each input\n",
    "        expanded_latent = latent.unsqueeze(1).expand(-1, max_len, -1)\n",
    "        \n",
    "        for t in range(max_len):\n",
    "            # Concatenate decoder input with latent vector for each time step\n",
    "            decoder_input_with_latent = torch.cat([decoder_input, latent.unsqueeze(1)], dim=2)\n",
    "            \n",
    "            # Feed through RNN\n",
    "            output, hidden = self.decoder_rnn(decoder_input_with_latent, hidden)\n",
    "            \n",
    "            # Project to vocabulary space\n",
    "            output = self.decoder_fc(output.squeeze(1))\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Teacher forcing: use ground truth as next input with probability\n",
    "            use_teacher_forcing = (torch.rand(1).item() < teacher_forcing_ratio)\n",
    "            \n",
    "            if use_teacher_forcing and t < max_len - 1:\n",
    "                # Use actual next token from target\n",
    "                decoder_input = self.embedding(tokens[:, t+1].unsqueeze(1))\n",
    "            else:\n",
    "                # Use own prediction\n",
    "                top1 = output.max(1)[1]\n",
    "                decoder_input = self.embedding(top1.unsqueeze(1))\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, tokens, token_lengths, numerical_features, teacher_forcing_ratio=0.5):\n",
    "        # Encode input to latent space\n",
    "        latent = self.encode(tokens, token_lengths, numerical_features)\n",
    "        \n",
    "        # Decode numerical features\n",
    "        reconstructed_numerical = self.decode_numerical(latent)\n",
    "        \n",
    "        # Decode token sequence\n",
    "        reconstructed_tokens = self.decode_tokens(latent, tokens, teacher_forcing_ratio)\n",
    "        \n",
    "        return reconstructed_numerical, reconstructed_tokens, latent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb988f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "hidden_dim = 128\n",
    "numerical_dim = len(dataset[0]['numerical_features'])\n",
    "latent_dim = 64  # Desired embedding dimension\n",
    "\n",
    "embedding_layer, vocab_size = load_word2vec_embedding(\"./models/word2vec.model\")\n",
    "\n",
    "# Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MTGCardAutoencoderWithTextDecoding(vocab_size, embedding_layer, hidden_dim, numerical_dim, latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss functions\n",
    "numerical_criterion = nn.MSELoss()\n",
    "token_criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens (index 0)\n",
    "alpha = 0.5  # Balance between numerical and token losses\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "def evaluate(model, val_dataloader, numerical_criterion, token_criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # Extract inputs from batch\n",
    "            tokens = batch['tokens'].to(device)\n",
    "            token_lengths = batch['token_lengths']\n",
    "            numerical_features = batch['numerical_features'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed_numerical, reconstructed_tokens, latent = model(\n",
    "                tokens, token_lengths, numerical_features, teacher_forcing_ratio=0.5\n",
    "            )\n",
    "            \n",
    "            # Compute numerical reconstruction loss\n",
    "            numerical_loss = numerical_criterion(reconstructed_numerical, numerical_features)\n",
    "            \n",
    "            # Compute token reconstruction loss\n",
    "            token_preds = reconstructed_tokens.view(-1, vocab_size)\n",
    "            token_targets = tokens.view(-1)\n",
    "            token_loss = token_criterion(token_preds, token_targets)\n",
    "            \n",
    "            # Combine losses\n",
    "            loss = alpha * numerical_loss + (1 - alpha) * token_loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a26d5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/50], Train Loss: 1.939921 - (Numerical: 0.030047, Token: 3.849796),  Val Loss:1.463849\n",
      "\n",
      "Epoch [2/50], Train Loss: 1.215432 - (Numerical: 0.015697, Token: 2.415167),  Val Loss:1.078084\n",
      "\n",
      "Epoch [3/50], Train Loss: 0.964046 - (Numerical: 0.012531, Token: 1.915561),  Val Loss:0.897568\n",
      "\n",
      "Epoch [4/50], Train Loss: 0.862638 - (Numerical: 0.010938, Token: 1.714338),  Val Loss:0.831638\n",
      "\n",
      "Epoch [5/50], Train Loss: 0.775891 - (Numerical: 0.009701, Token: 1.542081),  Val Loss:0.776536\n",
      "\n",
      "Epoch [6/50], Train Loss: 0.735041 - (Numerical: 0.008856, Token: 1.461226),  Val Loss:0.752625\n",
      "\n",
      "Epoch [7/50], Train Loss: 0.688282 - (Numerical: 0.008183, Token: 1.368382),  Val Loss:0.676089\n",
      "\n",
      "Epoch [8/50], Train Loss: 0.659735 - (Numerical: 0.007654, Token: 1.311815),  Val Loss:0.656277\n",
      "\n",
      "Epoch [9/50], Train Loss: 0.635300 - (Numerical: 0.007178, Token: 1.263422),  Val Loss:0.661609\n",
      "\n",
      "Epoch [10/50], Train Loss: 0.614003 - (Numerical: 0.006686, Token: 1.221320),  Val Loss:0.637570\n",
      "\n",
      "Epoch [11/50], Train Loss: 0.582145 - (Numerical: 0.006228, Token: 1.158061),  Val Loss:0.623064\n",
      "\n",
      "Epoch [12/50], Train Loss: 0.583353 - (Numerical: 0.006061, Token: 1.160645),  Val Loss:0.604599\n",
      "\n",
      "Epoch [13/50], Train Loss: 0.566025 - (Numerical: 0.005788, Token: 1.126262),  Val Loss:0.592846\n",
      "\n",
      "Epoch [14/50], Train Loss: 0.544508 - (Numerical: 0.005527, Token: 1.083489),  Val Loss:0.597553\n",
      "\n",
      "Epoch [15/50], Train Loss: 0.533987 - (Numerical: 0.005374, Token: 1.062599),  Val Loss:0.598745\n",
      "\n",
      "Epoch [16/50], Train Loss: 0.518875 - (Numerical: 0.005238, Token: 1.032512),  Val Loss:0.555960\n",
      "\n",
      "Epoch [17/50], Train Loss: 0.523759 - (Numerical: 0.005090, Token: 1.042428),  Val Loss:0.545834\n",
      "\n",
      "Epoch [18/50], Train Loss: 0.508368 - (Numerical: 0.004986, Token: 1.011750),  Val Loss:0.502268\n",
      "\n",
      "Epoch [19/50], Train Loss: 0.507801 - (Numerical: 0.004944, Token: 1.010658),  Val Loss:0.578920\n",
      "\n",
      "Epoch [20/50], Train Loss: 0.492261 - (Numerical: 0.004844, Token: 0.979678),  Val Loss:0.540081\n",
      "\n",
      "Epoch [21/50], Train Loss: 0.492901 - (Numerical: 0.004709, Token: 0.981093),  Val Loss:0.562445\n",
      "\n",
      "Epoch [22/50], Train Loss: 0.485647 - (Numerical: 0.004740, Token: 0.966554),  Val Loss:0.536330\n",
      "\n",
      "Epoch [23/50], Train Loss: 0.483399 - (Numerical: 0.004721, Token: 0.962078),  Val Loss:0.527995\n",
      "\n",
      "Epoch [24/50], Train Loss: 0.472521 - (Numerical: 0.004708, Token: 0.940335),  Val Loss:0.520738\n",
      "\n",
      "Epoch [25/50], Train Loss: 0.461080 - (Numerical: 0.004631, Token: 0.917529),  Val Loss:0.539476\n",
      "\n",
      "Epoch [26/50], Train Loss: 0.464903 - (Numerical: 0.004711, Token: 0.925094),  Val Loss:0.512616\n",
      "\n",
      "Epoch [27/50], Train Loss: 0.464733 - (Numerical: 0.004886, Token: 0.924581),  Val Loss:0.544631\n",
      "\n",
      "Epoch [28/50], Train Loss: 0.456593 - (Numerical: 0.004681, Token: 0.908506),  Val Loss:0.523886\n",
      "\n",
      "Epoch [29/50], Train Loss: 0.452473 - (Numerical: 0.004659, Token: 0.900287),  Val Loss:0.519981\n",
      "\n",
      "Epoch [30/50], Train Loss: 0.446983 - (Numerical: 0.004762, Token: 0.889203),  Val Loss:0.510704\n",
      "\n",
      "Epoch [31/50], Train Loss: 0.445441 - (Numerical: 0.004798, Token: 0.886084),  Val Loss:0.518846\n",
      "\n",
      "Epoch [32/50], Train Loss: 0.439394 - (Numerical: 0.004689, Token: 0.874098),  Val Loss:0.506539\n",
      "\n",
      "Epoch [33/50], Train Loss: 0.439565 - (Numerical: 0.004711, Token: 0.874419),  Val Loss:0.497634\n",
      "\n",
      "Epoch [34/50], Train Loss: 0.435389 - (Numerical: 0.004786, Token: 0.865993),  Val Loss:0.501201\n",
      "\n",
      "Epoch [35/50], Train Loss: 0.430427 - (Numerical: 0.004752, Token: 0.856102),  Val Loss:0.525519\n",
      "\n",
      "Epoch [36/50], Train Loss: 0.426800 - (Numerical: 0.004826, Token: 0.848775),  Val Loss:0.525430\n",
      "\n",
      "Epoch [37/50], Train Loss: 0.424339 - (Numerical: 0.004845, Token: 0.843834),  Val Loss:0.493982\n",
      "\n",
      "Epoch [38/50], Train Loss: 0.423400 - (Numerical: 0.004867, Token: 0.841934),  Val Loss:0.493223\n",
      "\n",
      "Epoch [39/50], Train Loss: 0.425416 - (Numerical: 0.004829, Token: 0.846004),  Val Loss:0.509387\n",
      "\n",
      "Epoch [40/50], Train Loss: 0.414625 - (Numerical: 0.004848, Token: 0.824402),  Val Loss:0.493560\n",
      "\n",
      "Epoch [41/50], Train Loss: 0.420343 - (Numerical: 0.004966, Token: 0.835720),  Val Loss:0.523196\n",
      "\n",
      "Epoch [42/50], Train Loss: 0.411232 - (Numerical: 0.004907, Token: 0.817556),  Val Loss:0.461488\n",
      "\n",
      "Epoch [43/50], Train Loss: 0.413994 - (Numerical: 0.004961, Token: 0.823026),  Val Loss:0.480297\n",
      "\n",
      "Epoch [44/50], Train Loss: 0.410029 - (Numerical: 0.004860, Token: 0.815198),  Val Loss:0.453599\n",
      "\n",
      "Epoch [45/50], Train Loss: 0.415165 - (Numerical: 0.005081, Token: 0.825249),  Val Loss:0.495709\n",
      "\n",
      "Epoch [46/50], Train Loss: 0.411533 - (Numerical: 0.004981, Token: 0.818085),  Val Loss:0.483230\n",
      "\n",
      "Epoch [47/50], Train Loss: 0.398980 - (Numerical: 0.004836, Token: 0.793124),  Val Loss:0.472152\n",
      "\n",
      "Epoch [48/50], Train Loss: 0.402577 - (Numerical: 0.004899, Token: 0.800255),  Val Loss:0.474215\n",
      "\n",
      "Epoch [49/50], Train Loss: 0.406407 - (Numerical: 0.004954, Token: 0.807860),  Val Loss:0.484333\n",
      "\n",
      "Epoch [50/50], Train Loss: 0.403993 - (Numerical: 0.005208, Token: 0.802779),  Val Loss:0.472119\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    numerical_losses = 0\n",
    "    token_losses = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        tokens = batch['tokens'].to(device)\n",
    "        token_lengths = batch['token_lengths']\n",
    "        numerical_features = batch['numerical_features'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed_numerical, reconstructed_tokens, latent = model(\n",
    "            tokens, token_lengths, numerical_features, teacher_forcing_ratio=0.5\n",
    "        )\n",
    "        \n",
    "        # Compute numerical reconstruction loss\n",
    "        numerical_loss = numerical_criterion(reconstructed_numerical, numerical_features)\n",
    "        \n",
    "        # Compute token reconstruction loss\n",
    "        # Reshape predictions to [batch_size*seq_len, vocab_size]\n",
    "        token_preds = reconstructed_tokens.view(-1, vocab_size)\n",
    "        # Reshape targets to [batch_size*seq_len]\n",
    "        token_targets = tokens.view(-1)\n",
    "        token_loss = token_criterion(token_preds, token_targets)\n",
    "        \n",
    "        # Combine losses        \n",
    "        loss = alpha * numerical_loss + (1 - alpha) * token_loss\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        numerical_losses += numerical_loss.item()\n",
    "        token_losses += token_loss.item()\n",
    "    \n",
    "    val_loss = evaluate(model, val_dataloader, numerical_criterion, token_criterion, device)\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {total_loss/len(train_dataloader):.6f} - (\"\n",
    "          f\"Numerical: {numerical_losses/len(train_dataloader):.6f}, \"\n",
    "          f\"Token: {token_losses/len(train_dataloader):.6f}), \"\n",
    "          f\" Val Loss:{val_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0af10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/AE_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a968fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benwm\\AppData\\Local\\Temp\\ipykernel_17376\\1658437441.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"./models/AE_weights.pt\", map_location=torch.device('cuda'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(\"./models/AE_weights.pt\", map_location=torch.device('cuda'))\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c1f4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all cards\n",
    "model.eval()\n",
    "all_embeddings = []\n",
    "all_names = []\n",
    "all_imgs = []\n",
    "\n",
    "dataset = MTGCardDataset(df)  # df is your pandas dataframe\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_mtg_cards\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        tokens = batch['tokens'].to(device)\n",
    "        token_lengths = batch['token_lengths']\n",
    "        numerical_features = batch['numerical_features'].to(device)\n",
    "        names = batch['names']\n",
    "        imgs = batch['imgs']\n",
    "        \n",
    "        _, _, latent = model(tokens, token_lengths, numerical_features)\n",
    "        \n",
    "        all_embeddings.append(latent.cpu().numpy())\n",
    "        all_names.extend(names)\n",
    "        all_imgs.extend(imgs)\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cb38da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings and names to saved_embeddings/mtg_embeddings_20250808.json\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_embeddings(embeddings, names, imgs, directory=\"saved_embeddings\"):\n",
    "    \"\"\"\n",
    "    Save embeddings and corresponding card names using various methods\n",
    "    \n",
    "    Args:\n",
    "        embeddings: NumPy array of embeddings\n",
    "        names: List of card names\n",
    "        directory: Directory to save files in\n",
    "    \"\"\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "    filename = f\"{directory}/mtg_embeddings_{timestamp}.json\"\n",
    "    \n",
    "    data = {\n",
    "        'embeddings': embeddings.tolist(),\n",
    "        'names': names,\n",
    "        'imgs': imgs,\n",
    "        'metadata': {\n",
    "            'date': timestamp,\n",
    "            'embedding_dim': embeddings.shape[1],\n",
    "            'num_cards': len(names)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    print(f\"Saved embeddings and names to {filename}\")\n",
    "    return filename\n",
    "\n",
    "save_path = save_embeddings(all_embeddings, all_names, all_imgs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
